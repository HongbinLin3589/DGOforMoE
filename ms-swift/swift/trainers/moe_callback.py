# Copyright (c) DGO Project
# MoE Monitor Callback - ä¸“é—¨é’ˆå¯¹ 4 ä¸ªç›®æ ‡æ¨¡å‹
# =============================================================================
# æ”¯æŒçš„æ¨¡å‹:
# 1. OLMoE-1B-7B-0125 (Allen AI)
# 2. Qwen1.5-MoE-A2.7B (Alibaba)
# 3. deepseek-moe-16b-base (DeepSeek)
# 4. Mixtral-8x7B-v0.1 (Mistral AI)
# =============================================================================

import os
import re
import sys
import torch
from pathlib import Path
from typing import Dict, Optional, List, Tuple
from dataclasses import dataclass
from transformers import TrainerCallback, TrainerControl, TrainerState
from transformers.training_args import TrainingArguments
from swift.utils import get_logger

logger = get_logger()

sys.path.insert(0, str(Path(__file__).parent.parent.parent.parent))

try:
    from moe_monitor import MoEMonitor
except ImportError:
    MoEMonitor = None


# =============================================================================
# æ‚¨çš„ 4 ä¸ªæ¨¡å‹çš„ç²¾ç¡®é…ç½®
# =============================================================================

@dataclass
class MoEConfig:
    """MoE æ¨¡å‹é…ç½®"""
    num_experts: int           # routed ä¸“å®¶æ•°
    topk: int                  # æ¯ä¸ª token æ¿€æ´»çš„ä¸“å®¶æ•°
    num_layers: int            # MoE å±‚æ•°
    has_shared_expert: bool    # æ˜¯å¦æœ‰ shared expert
    gate_attr_name: str        # gate æ¨¡å—åœ¨ MoE block ä¸­çš„å±æ€§å
    config_expert_attr: str    # model.config ä¸­ä¸“å®¶æ•°çš„å±æ€§å
    config_topk_attr: str      # model.config ä¸­ topk çš„å±æ€§å


# ç²¾ç¡®é…ç½® - åŸºäº HuggingFace æ¨¡å‹å®šä¹‰
MODEL_CONFIGS = {
    # =========================================================================
    # OLMoE-1B-7B-0125
    # ç»“æ„: model.model.layers[i].mlp.gate
    # Config: num_experts=64, num_experts_per_tok=8
    # =========================================================================
    "olmoe": MoEConfig(
        num_experts=64,
        topk=8,
        num_layers=16,
        has_shared_expert=False,
        gate_attr_name="gate",           # layers[i].mlp.gate
        config_expert_attr="num_experts",
        config_topk_attr="num_experts_per_tok",
    ),

    # =========================================================================
    # Qwen1.5-MoE-A2.7B
    # ç»“æ„: model.model.layers[i].mlp.gate (Qwen2MoE æ¶æ„)
    # Config: num_experts=60, num_experts_per_tok=4
    # ç‰¹ç‚¹: æœ‰ 1 ä¸ª shared_expert + shared_expert_gate
    # =========================================================================
    "qwen": MoEConfig(
        num_experts=60,       # 60 routed experts
        topk=4,               # top-4 routing
        num_layers=24,
        has_shared_expert=True,  # æœ‰ shared expert
        gate_attr_name="gate",   # layers[i].mlp.gate
        config_expert_attr="num_experts",
        config_topk_attr="num_experts_per_tok",
    ),

    # =========================================================================
    # deepseek-moe-16b-base
    # ç»“æ„: model.model.layers[i].mlp.gate (DeepseekMoE æ¶æ„)
    # Config: n_routed_experts=64, num_experts_per_tok=6
    # ç‰¹ç‚¹: 64 routed + 2 shared experts, 28 layers
    # =========================================================================
    "deepseek": MoEConfig(
        num_experts=64,       # 64 routed experts
        topk=6,               # top-6 routing (ä»è®ºæ–‡ç¡®è®¤)
        num_layers=28,        # 28 transformer layers
        has_shared_expert=True,  # 2 shared experts
        gate_attr_name="gate",   # layers[i].mlp.gate
        config_expert_attr="n_routed_experts",
        config_topk_attr="num_experts_per_tok",
    ),

    # =========================================================================
    # Mixtral-8x7B-v0.1
    # ç»“æ„: model.model.layers[i].block_sparse_moe.gate
    # Config: num_local_experts=8, num_experts_per_tok=2
    # =========================================================================
    "mixtral": MoEConfig(
        num_experts=8,        # 8 experts
        topk=2,               # top-2 routing
        num_layers=32,
        has_shared_expert=False,
        gate_attr_name="gate",   # block_sparse_moe.gate
        config_expert_attr="num_local_experts",
        config_topk_attr="num_experts_per_tok",
    ),
}


class MoEMonitorCallback(TrainerCallback):
    """
    MoE Monitor Callback - é’ˆå¯¹ 4 ä¸ªç›®æ ‡æ¨¡å‹ä¼˜åŒ–

    æ”¯æŒ:
    - OLMoE-1B-7B-0125: 64 experts, top-8
    - Qwen1.5-MoE-A2.7B: 60 experts, top-4, +shared
    - deepseek-moe-16b-base: 64 experts, top-6, +2 shared
    - Mixtral-8x7B-v0.1: 8 experts, top-2

    ä½¿ç”¨æ–¹æ³•:
    ```python
    callback = MoEMonitorCallback(
        model_key="olmoe",  # æˆ– "qwen", "deepseek", "mixtral"
        log_every=100,
    )
    trainer = Trainer(model=model, callbacks=[callback], ...)
    ```
    """

    def __init__(
        self,
        model_key: Optional[str] = None,  # "olmoe", "qwen", "deepseek", "mixtral"
        ref_model: Optional[torch.nn.Module] = None,
        log_every: int = 100,
        save_dir: str = "./moe_logs",
        enabled: bool = True,
        track_per_layer: bool = False,
        verbose: bool = True,
    ):
        super().__init__()

        self.enabled = enabled and MoEMonitor is not None
        if not self.enabled:
            if MoEMonitor is None:
                logger.warning("âš ï¸  MoEMonitor not found. MoE monitoring disabled.")
            return

        self.model_key = model_key
        self.ref_model = ref_model
        self.log_every = log_every
        self.save_dir = save_dir
        self.track_per_layer = track_per_layer
        self.verbose = verbose
        self.monitor = None

        # é…ç½® (å°†åœ¨ on_train_begin ä¸­è®¾ç½®)
        self.moe_config: Optional[MoEConfig] = None
        self.detected_model: str = "unknown"

        # Hook ç®¡ç†
        self.router_logits_cache: List[torch.Tensor] = []
        self.layer_indices_cache: List[int] = []
        self.hook_handles = []
        self._last_logged_step = -1
        self._latest_metrics = {}  # å­˜å‚¨æœ€æ–°çš„MoEæŒ‡æ ‡ä¾›on_logä½¿ç”¨

    def on_train_begin(
        self,
        args: TrainingArguments,
        state: TrainerState,
        control: TrainerControl,
        model=None,
        **kwargs
    ):
        """åˆå§‹åŒ– MoE monitor å¹¶æ³¨å†Œ hooks"""
        if not self.enabled or model is None:
            return

        # è®¾ç½®ä¿å­˜ç›®å½•
        self.save_dir = os.path.join(args.output_dir, "moe_logs")

        # =====================================================================
        # æ£€æµ‹æ¨¡å‹ç±»å‹å¹¶è·å–é…ç½®
        # =====================================================================
        self.moe_config, self.detected_model = self._detect_and_configure(model)

        if self.moe_config is None:
            logger.warning("âš ï¸  æ— æ³•æ£€æµ‹ MoE æ¶æ„ï¼Œç¦ç”¨ç›‘æ§")
            self.enabled = False
            return

        # =====================================================================
        # åˆå§‹åŒ– monitor
        # =====================================================================
        try:
            self.monitor = MoEMonitor(
                model=model,
                ref_model=self.ref_model,
                save_dir=self.save_dir,
                num_experts=self.moe_config.num_experts,
                topk=self.moe_config.topk,
                num_layers=self.moe_config.num_layers,
            )

            if self.verbose:
                self._print_config()

        except Exception as e:
            logger.warning(f"âš ï¸  åˆå§‹åŒ– MoE Monitor å¤±è´¥: {e}")
            self.enabled = False
            return

        # å¯ç”¨ router logits è¾“å‡º
        if hasattr(model, 'config'):
            model.config.output_router_logits = True

        # =====================================================================
        # æ³¨å†Œ hooks
        # =====================================================================
        self._register_hooks(model)

    def _detect_and_configure(self, model) -> Tuple[Optional[MoEConfig], str]:
        """æ£€æµ‹æ¨¡å‹ç±»å‹å¹¶è¿”å›é…ç½®"""

        # å¦‚æœç”¨æˆ·æ˜ç¡®æŒ‡å®šäº† model_key
        if self.model_key and self.model_key in MODEL_CONFIGS:
            config = MODEL_CONFIGS[self.model_key]
            # ç”¨å®é™… model.config çš„å€¼è¦†ç›–é»˜è®¤å€¼
            return self._override_from_model(model, config, self.model_key), self.model_key

        # è‡ªåŠ¨æ£€æµ‹
        model_config = getattr(model, 'config', None)
        if model_config is None:
            return None, "unknown"

        # è·å–æ ‡è¯†ç¬¦
        model_type = getattr(model_config, 'model_type', '').lower()
        name_or_path = getattr(model_config, '_name_or_path', '').lower()

        # åŒ¹é…æ¨¡å‹
        detected_key = None

        if 'olmoe' in model_type or 'olmoe' in name_or_path:
            detected_key = "olmoe"
        elif 'qwen2_moe' in model_type or 'qwen1.5-moe' in name_or_path or ('qwen' in name_or_path and 'moe' in name_or_path):
            detected_key = "qwen"
        elif 'deepseek' in model_type or 'deepseek' in name_or_path:
            detected_key = "deepseek"
        elif 'mixtral' in model_type or 'mixtral' in name_or_path:
            detected_key = "mixtral"

        if detected_key is None:
            logger.warning(f"âš ï¸  æ— æ³•è¯†åˆ«æ¨¡å‹ç±»å‹: model_type={model_type}, name={name_or_path}")
            return None, "unknown"

        config = MODEL_CONFIGS[detected_key]
        return self._override_from_model(model, config, detected_key), detected_key

    def _override_from_model(
        self,
        model,
        base_config: MoEConfig,
        model_key: str
    ) -> MoEConfig:
        """ç”¨ model.config çš„å®é™…å€¼è¦†ç›–åŸºç¡€é…ç½®"""
        model_config = getattr(model, 'config', None)
        if model_config is None:
            return base_config

        # è¯»å–å®é™…å€¼
        num_experts = getattr(model_config, base_config.config_expert_attr, base_config.num_experts)
        topk = getattr(model_config, base_config.config_topk_attr, base_config.topk)
        num_layers = getattr(model_config, 'num_hidden_layers', base_config.num_layers)

        return MoEConfig(
            num_experts=num_experts,
            topk=topk,
            num_layers=num_layers,
            has_shared_expert=base_config.has_shared_expert,
            gate_attr_name=base_config.gate_attr_name,
            config_expert_attr=base_config.config_expert_attr,
            config_topk_attr=base_config.config_topk_attr,
        )

    def _print_config(self):
        """æ‰“å°é…ç½®ä¿¡æ¯"""
        logger.info(f"\n{'='*60}")
        logger.info(f"âœ… MoE Monitor å·²åˆå§‹åŒ–")
        logger.info(f"{'='*60}")
        logger.info(f"   æ¨¡å‹:           {self.detected_model}")
        logger.info(f"   ä¸“å®¶æ•°:         {self.moe_config.num_experts}")
        logger.info(f"   Top-K:          {self.moe_config.topk}")
        logger.info(f"   å±‚æ•°:           {self.moe_config.num_layers}")
        logger.info(f"   Shared Expert:  {self.moe_config.has_shared_expert}")
        logger.info(f"   ä¿å­˜ç›®å½•:       {self.save_dir}")
        logger.info(f"   æ—¥å¿—é—´éš”:       {self.log_every} steps")
        logger.info(f"{'='*60}\n")

    def _register_hooks(self, model):
        """æ³¨å†Œ forward hooks æ•è· router logits"""

        # âœ… FIX: å¤„ç† DDP wrapped model
        # å¦‚æœæ¨¡å‹è¢« DistributedDataParallel åŒ…è£…ï¼Œéœ€è¦è®¿é—®å†…éƒ¨çš„ module
        if hasattr(model, 'module'):
            actual_model = model.module
            if self.verbose:
                logger.info(f"   æ£€æµ‹åˆ° DDP åŒ…è£…ï¼Œä½¿ç”¨ model.module")
        else:
            actual_model = model

        hooked_count = 0

        def make_hook(layer_idx):
            def router_hook(module, input, output):
                # âœ… FIX: å¤„ç† tuple è¾“å‡ºï¼ˆæŸäº›æ¨¡å‹çš„ gate è¿”å› (logits, aux_loss)ï¼‰
                if isinstance(output, tuple):
                    output = output[0]  # é€šå¸¸ç¬¬ä¸€ä¸ªå…ƒç´ æ˜¯ logits

                if isinstance(output, torch.Tensor) and output.dim() == 2:
                    # éªŒè¯å½¢çŠ¶: [batch*seq, num_experts]
                    if output.shape[-1] == self.moe_config.num_experts:
                        self.router_logits_cache.append(output.detach().cpu())
                        self.layer_indices_cache.append(layer_idx)
            return router_hook

        # æ ¹æ®æ¨¡å‹ç±»å‹ç¡®å®š gate æ¨¡å—è·¯å¾„
        gate_patterns = self._get_gate_patterns()

        # ä½¿ç”¨ actual_model è€Œä¸æ˜¯ model
        for name, module in actual_model.named_modules():
            # æ£€æŸ¥æ˜¯å¦åŒ¹é… gate æ¨¡å¼
            if any(name.endswith(pattern) for pattern in gate_patterns):
                layer_idx = self._extract_layer_index(name)
                handle = module.register_forward_hook(make_hook(layer_idx))
                self.hook_handles.append(handle)
                hooked_count += 1
                if self.verbose and hooked_count <= 3:
                    logger.info(f"   Hook: {name} (layer {layer_idx})")

        if hooked_count > 0:
            if self.verbose:
                logger.info(f"   âœ… æ³¨å†Œäº† {hooked_count} ä¸ª gate hooks")
        else:
            logger.info(f"   âš ï¸  æœªæ‰¾åˆ° gate æ¨¡å—ï¼Œå°è¯•çš„æ¨¡å¼: {gate_patterns}")
            self.enabled = False

    def _get_gate_patterns(self) -> List[str]:
        """æ ¹æ®æ¨¡å‹ç±»å‹è¿”å› gate æ¨¡å—çš„åŒ¹é…æ¨¡å¼"""
        patterns = {
            "olmoe": [".mlp.gate"],
            "qwen": [".mlp.gate"],
            "deepseek": [".mlp.gate"],
            "mixtral": [".block_sparse_moe.gate"],
        }
        return patterns.get(self.detected_model, [".gate", ".mlp.gate"])

    def _extract_layer_index(self, module_name: str) -> int:
        """ä»æ¨¡å—åæå–å±‚ç´¢å¼•"""
        match = re.search(r'layers\.(\d+)', module_name)
        if match:
            return int(match.group(1))
        return -1

    def on_step_end(
        self,
        args: TrainingArguments,
        state: TrainerState,
        control: TrainerControl,
        model=None,
        **kwargs
    ):
        """åœ¨æ¯ä¸ª step ç»“æŸåè®¡ç®—å¹¶è®°å½• MoE æŒ‡æ ‡åˆ° TensorBoard"""
        if not self.enabled or self.monitor is None:
            return

        # âœ… FIX: åˆ¤æ–­æ˜¯å¦éœ€è¦è®°å½•ï¼Œé¿å…æ—©æœŸ return å¯¼è‡´ç¼“å­˜ä¸æ¸…ç©ºï¼ˆå†…å­˜æ³„æ¼ï¼‰
        should_log = (state.global_step % self.log_every == 0 and
                      state.global_step != self._last_logged_step)

        try:
            # âœ… åªåœ¨ä¸»è¿›ç¨‹ä¸”éœ€è¦è®°å½•æ—¶æ‰è®¡ç®—
            if should_log and state.is_world_process_zero and self.router_logits_cache:
                self._last_logged_step = state.global_step

                # è®¡ç®—æŒ‡æ ‡
                metrics = self._compute_metrics()

                if metrics:
                    # ä¿å­˜æœ€æ–°æŒ‡æ ‡ä¾› on_log ä½¿ç”¨ï¼ˆç”¨äºTensorBoardè®°å½•ï¼‰
                    self._latest_metrics = metrics

                    # ä¿å­˜åˆ° monitor (åªå†™æ–‡ä»¶ï¼Œä¸ä¼šé˜»å¡)
                    self.monitor.log(state.global_step, metrics)

                    # âœ… FIX: ç›´æ¥è®°å½•MoEæŒ‡æ ‡åˆ°logging.jsonl/TensorBoard
                    # é—®é¢˜: on_log callback æœªè¢«DGO/GRPO trainerè°ƒç”¨
                    # è§£å†³: ç›´æ¥å°†MoEæŒ‡æ ‡æ·»åŠ åˆ°state.log_historyï¼Œè®©traineråœ¨ä¿å­˜logsæ—¶è‡ªåŠ¨åŒ…å«
                    # Note: MoE metrics are now properly logged to TensorBoard via on_log callback
                    # The callback is inserted at position 0 to ensure on_log runs before TensorBoard

                    # æ¯50æ­¥è‡ªåŠ¨ä¿å­˜ä¸€æ¬¡ï¼Œé¿å…è®­ç»ƒä¸­æ–­ä¸¢å¤±æ•°æ®
                    if state.global_step % 50 == 0:
                        try:
                            self.monitor.save()
                        except Exception as e:
                            logger.warning(f"âš ï¸  è‡ªåŠ¨ä¿å­˜ MoE æŒ‡æ ‡å¤±è´¥: {e}")

                    # æ‰“å°æ‘˜è¦
                    metric_str = " | ".join([f"{k}={v:.4f}" for k, v in metrics.items()])
                    logger.info(f"\nğŸ“Š [Step {state.global_step}] MoE ({self.detected_model}): {metric_str}")

                    # è­¦å‘Š
                    if metrics.get('collapse_rate', 0) > 0.3:
                        logger.info(f"   âš ï¸  é«˜å´©æºƒç‡: {metrics['collapse_rate']:.1%}")
                    if metrics.get('load_cv', 0) > 1.0:
                        logger.info(f"   âš ï¸  é«˜è´Ÿè½½ä¸å‡è¡¡: CV={metrics['load_cv']:.2f}")

        except Exception as e:
            logger.warning(f"âš ï¸  è®¡ç®— MoE æŒ‡æ ‡æ—¶å‡ºé”™ (step {state.global_step}): {e}")
            import traceback
            traceback.print_exc()
        finally:
            # âœ… FIX: ç¡®ä¿æ€»æ˜¯æ¸…ç©ºç¼“å­˜ï¼ˆä¿®å¤å†…å­˜æ³„æ¼å’Œé”™è¯¯æ¢å¤ï¼‰
            # æ‰€æœ‰è¿›ç¨‹éƒ½éœ€è¦æ¸…ç©ºç¼“å­˜ï¼Œé¿å…å†…å­˜ç§¯ç´¯
            self.router_logits_cache = []
            self.layer_indices_cache = []

    def _compute_metrics(self) -> Dict[str, float]:
        """è®¡ç®—æ‰€æœ‰ 4 ä¸ªæŒ‡æ ‡ï¼ˆä½¿ç”¨ bincount ä¼˜åŒ–ï¼‰"""
        if not self.router_logits_cache:
            return {}

        try:
            metrics = {}

            # è®¡ç®— Load CV
            avg_cv, layer_cvs = self._compute_load_cv()
            metrics['load_cv'] = avg_cv

            if self.track_per_layer and layer_cvs:
                for layer_idx, cv in layer_cvs.items():
                    metrics[f'layer_{layer_idx}_cv'] = cv

            # è®¡ç®—å´©æºƒç‡
            metrics['collapse_rate'] = self._compute_collapse_rate()

            # è®¡ç®—è·¯ç”±ç†µ (Routing Entropy)
            metrics['routing_entropy'] = self._compute_routing_entropy()

            # è®¡ç®—æœ€å¤§è´Ÿè½½ (Max Load)
            metrics['max_load'] = self._compute_max_load()

            return metrics

        except Exception as e:
            logger.warning(f"âš ï¸  _compute_metrics å‡ºé”™: {e}")
            return {}

    def _compute_load_cv(self) -> Tuple[float, Dict[int, float]]:
        """è®¡ç®—è´Ÿè½½å˜å¼‚ç³»æ•°

        ä¿®å¤ï¼šå…ˆæŒ‰å±‚ç´¯ç§¯æ‰€æœ‰ batch çš„ expert countsï¼Œå†è®¡ç®— CV

        ä¹‹å‰çš„ bugï¼šæ¯ä¸ª batch å•ç‹¬ç®— CV å†å¹³å‡ï¼Œä¼šå¯¼è‡´ CV è¢«äººä¸ºæ”¾å¤§
        ä¾‹å¦‚ï¼šbatch1 ä¸“å®¶0è´Ÿè½½é«˜(CV=1.5)ï¼Œbatch2 ä¸“å®¶1è´Ÿè½½é«˜(CV=1.5)
              é”™è¯¯æ–¹æ³•: avg_cv = 1.5
              æ­£ç¡®æ–¹æ³•: åˆå¹¶åå‡è¡¡ï¼Œcv = 0.2
        """
        from collections import defaultdict

        # Step 1: æŒ‰å±‚ç´¯ç§¯æ‰€æœ‰ batch çš„ expert counts
        layer_counts = defaultdict(lambda: torch.zeros(self.moe_config.num_experts))

        for logits, layer_idx in zip(self.router_logits_cache, self.layer_indices_cache):
            # logits: [N, num_experts]
            topk_indices = torch.topk(logits, self.moe_config.topk, dim=-1).indices
            flat_indices = topk_indices.flatten()

            # ä½¿ç”¨ bincount è®¡æ•° (æ¯”å¾ªç¯å¿« 100x)
            expert_counts = torch.bincount(
                flat_indices,
                minlength=self.moe_config.num_experts
            ).float()

            # âœ… ç´¯ç§¯åˆ°åŒä¸€å±‚ï¼ˆè€Œä¸æ˜¯æ¯ä¸ªbatchå•ç‹¬è®¡ç®—ï¼‰
            layer_counts[layer_idx] += expert_counts

        # Step 2: æ¯å±‚è®¡ç®—ä¸€æ¬¡ CV
        layer_cvs = {}
        for layer_idx, counts in layer_counts.items():
            total = counts.sum()
            if total == 0:
                continue

            load = counts / total
            cv = (load.std() / (load.mean() + 1e-8)).item()
            layer_cvs[layer_idx] = cv

        # Step 3: è¿”å›æ‰€æœ‰å±‚çš„å¹³å‡ CV
        avg_cv = sum(layer_cvs.values()) / len(layer_cvs) if layer_cvs else 0.0

        return avg_cv, layer_cvs

    def _compute_collapse_rate(self, threshold: float = 0.01) -> float:
        """è®¡ç®—è·¯ç”±å´©æºƒç‡

        Args:
            threshold: ç»å¯¹é˜ˆå€¼ï¼Œè´Ÿè½½ < threshold çš„ä¸“å®¶è¢«è®¤ä¸º"å´©æºƒ"
                      é»˜è®¤ 0.01 (1%)ï¼Œå³è´Ÿè½½ä½äº 1% çš„ä¸“å®¶ç®—å´©æºƒ

        ä¿®å¤ï¼šæŒ‰å±‚ç´¯ç§¯åè®¡ç®—æ¯å±‚çš„å´©æºƒç‡ï¼Œè¿”å›æœ€å¤§å€¼ï¼ˆæœ€å·®æƒ…å†µï¼‰
        ä¹‹å‰çš„ bugï¼šæ··åˆæ‰€æœ‰å±‚ï¼ŒæŸå±‚å´©æºƒä¼šè¢«å…¶ä»–å±‚æ©ç›–
        """
        from collections import defaultdict

        # Step 1: æŒ‰å±‚ç´¯ç§¯ counts
        layer_counts = defaultdict(lambda: torch.zeros(self.moe_config.num_experts))

        for logits, layer_idx in zip(self.router_logits_cache, self.layer_indices_cache):
            topk_indices = torch.topk(logits, self.moe_config.topk, dim=-1).indices
            flat_indices = topk_indices.flatten()

            expert_counts = torch.bincount(
                flat_indices,
                minlength=self.moe_config.num_experts
            ).float()

            layer_counts[layer_idx] += expert_counts

        # Step 2: æ¯å±‚è®¡ç®—å´©æºƒç‡
        layer_collapse_rates = []
        for counts in layer_counts.values():
            total = counts.sum()
            if total == 0:
                continue

            load = counts / total
            collapsed = (load < threshold).sum().item()
            collapse_rate = collapsed / self.moe_config.num_experts
            layer_collapse_rates.append(collapse_rate)

        # Step 3: è¿”å›æœ€å¤§å´©æºƒç‡ï¼ˆåæ˜ æœ€å·®çš„å±‚ï¼‰
        return max(layer_collapse_rates) if layer_collapse_rates else 0.0

    def _compute_routing_entropy(self) -> float:
        """
        è®¡ç®—è·¯ç”±ç†µ (Routing Entropy)

        ç†µè¶Šé«˜è¡¨ç¤ºè·¯ç”±åˆ†å¸ƒè¶Šå‡åŒ€ï¼Œè¶Šä½è¡¨ç¤ºè¶Šé›†ä¸­
        ç†æƒ³æƒ…å†µä¸‹: entropy = log(num_experts) è¡¨ç¤ºå®Œå…¨å‡åŒ€

        ä¿®å¤ï¼šæŒ‰å±‚ç´¯ç§¯åè®¡ç®—æ¯å±‚çš„ç†µï¼Œå†å–å¹³å‡
        ä¹‹å‰çš„ bugï¼šæ··åˆæ‰€æœ‰å±‚ï¼ŒæŸå±‚å´©æºƒä¼šè¢«å…¶ä»–å±‚æ©ç›–
        """
        from collections import defaultdict

        # Step 1: æŒ‰å±‚ç´¯ç§¯ counts
        layer_counts = defaultdict(lambda: torch.zeros(self.moe_config.num_experts))

        for logits, layer_idx in zip(self.router_logits_cache, self.layer_indices_cache):
            topk_indices = torch.topk(logits, self.moe_config.topk, dim=-1).indices
            flat_indices = topk_indices.flatten()

            expert_counts = torch.bincount(
                flat_indices,
                minlength=self.moe_config.num_experts
            ).float()

            layer_counts[layer_idx] += expert_counts

        # Step 2: æ¯å±‚è®¡ç®—ç†µ
        max_entropy = torch.log(torch.tensor(float(self.moe_config.num_experts))).item()
        layer_entropies = []

        for counts in layer_counts.values():
            total = counts.sum()
            if total == 0:
                continue

            probs = counts / total
            probs = probs[probs > 0]

            entropy = -torch.sum(probs * torch.log(probs + 1e-10)).item()
            normalized = entropy / max_entropy if max_entropy > 0 else 0.0
            layer_entropies.append(normalized)

        # Step 3: è¿”å›å¹³å‡ç†µ
        return sum(layer_entropies) / len(layer_entropies) if layer_entropies else 0.0

    def _compute_max_load(self) -> float:
        """
        è®¡ç®—æœ€å¤§ä¸“å®¶è´Ÿè½½

        è¡¨ç¤ºæœ€ç¹å¿™ä¸“å®¶æ‰¿æ‹…çš„ token æ¯”ä¾‹
        ç†æƒ³æƒ…å†µä¸‹: max_load = 1/num_experts (å®Œå…¨å‡è¡¡)
        å€¼è¶Šé«˜è¡¨ç¤ºè´Ÿè½½è¶Šä¸å‡è¡¡

        ä¿®å¤ï¼šæŒ‰å±‚ç´¯ç§¯åè®¡ç®—æ¯å±‚çš„ max_loadï¼Œè¿”å›æœ€å¤§å€¼ï¼ˆæœ€å·®æƒ…å†µï¼‰
        ä¹‹å‰çš„ bugï¼šæ··åˆæ‰€æœ‰å±‚åå– maxï¼Œå³°å€¼è¢«å¹³æ»‘äº†
        """
        from collections import defaultdict

        # Step 1: æŒ‰å±‚ç´¯ç§¯ counts
        layer_counts = defaultdict(lambda: torch.zeros(self.moe_config.num_experts))

        for logits, layer_idx in zip(self.router_logits_cache, self.layer_indices_cache):
            topk_indices = torch.topk(logits, self.moe_config.topk, dim=-1).indices
            flat_indices = topk_indices.flatten()

            expert_counts = torch.bincount(
                flat_indices,
                minlength=self.moe_config.num_experts
            ).float()

            layer_counts[layer_idx] += expert_counts

        # Step 2: æ¯å±‚è®¡ç®— max_loadï¼Œå–æœ€å¤§å€¼ï¼ˆæœ€å·®æƒ…å†µï¼‰
        layer_max_loads = []
        for counts in layer_counts.values():
            total = counts.sum()
            if total == 0:
                continue

            load = counts / total
            layer_max_loads.append(load.max().item())

        # Step 3: è¿”å›æ‰€æœ‰å±‚ä¸­çš„æœ€å¤§å€¼ï¼ˆåæ˜ æœ€ä¸å‡è¡¡çš„å±‚ï¼‰
        return max(layer_max_loads) if layer_max_loads else 0.0

    def on_log(
        self,
        args: TrainingArguments,
        state: TrainerState,
        control: TrainerControl,
        logs: Dict[str, float] = None,
        **kwargs
    ):
        """
        å°†MoEæŒ‡æ ‡æ·»åŠ åˆ°trainerçš„æ—¥å¿—ä¸­ï¼Œä½¿å…¶è®°å½•åˆ°TensorBoardã€‚

        æ³¨æ„ï¼šon_logåœ¨æ‰€æœ‰è¿›ç¨‹åŒæ­¥åè°ƒç”¨ï¼Œä¿®æ”¹logså­—å…¸æ˜¯å®‰å…¨çš„ã€‚
        """
        if not self.enabled or logs is None:
            return

        # åªåœ¨ä¸»è¿›ç¨‹æ·»åŠ æŒ‡æ ‡
        if not state.is_world_process_zero:
            return

        # å°†æœ€æ–°çš„MoEæŒ‡æ ‡æ·»åŠ åˆ°logsï¼ŒTensorBoardä¼šè‡ªåŠ¨è®°å½•
        if hasattr(self, '_latest_metrics') and self._latest_metrics:
            for key, value in self._latest_metrics.items():
                # æ·»åŠ å‰ç¼€ä»¥ä¾¿åœ¨TensorBoardä¸­åˆ†ç»„æ˜¾ç¤º
                logs[f'moe/{key}'] = value

    def on_train_end(
        self,
        args: TrainingArguments,
        state: TrainerState,
        control: TrainerControl,
        **kwargs
    ):
        """æ¸…ç†å¹¶ä¿å­˜"""
        if not self.enabled:
            return

        # ç§»é™¤ hooks (æ‰€æœ‰è¿›ç¨‹éƒ½éœ€è¦æ‰§è¡Œ)
        for handle in self.hook_handles:
            handle.remove()
        self.hook_handles = []
        self.router_logits_cache = []
        self.layer_indices_cache = []

        # åªåœ¨ä¸»è¿›ç¨‹ä¿å­˜æŒ‡æ ‡å’Œæ‰“å°æ—¥å¿—
        if not state.is_world_process_zero:
            return

        if self.monitor is not None:
            try:
                self.monitor.save()

                if self.verbose:
                    logger.info(f"\n{'='*60}")
                    logger.info(f"âœ… MoE è®­ç»ƒå®Œæˆ - {self.detected_model}")
                    logger.info(f"{'='*60}")
                    logger.info(f"   æŒ‡æ ‡å·²ä¿å­˜åˆ°: {self.save_dir}/moe_metrics.json")

                    if self.monitor.history.get('step'):
                        for metric in ['load_cv', 'collapse_rate']:
                            if metric in self.monitor.history:
                                values = self.monitor.history[metric]
                                if values:
                                    logger.info(f"   {metric}: min={min(values):.4f}, "
                                          f"max={max(values):.4f}, final={values[-1]:.4f}")
                    logger.info(f"{'='*60}\n")

            except Exception as e:
                logger.warning(f"âš ï¸  ä¿å­˜ MoE æŒ‡æ ‡å¤±è´¥: {e}")


# =============================================================================
# ä¾¿æ·å‡½æ•°
# =============================================================================

def get_moe_callback(
    model_key: Optional[str] = None,
    log_every: int = 100,
    **kwargs
) -> MoEMonitorCallback:
    """
    è·å– MoE ç›‘æ§ Callback

    Args:
        model_key: æ¨¡å‹æ ‡è¯†ç¬¦
            - "olmoe": OLMoE-1B-7B-0125
            - "qwen": Qwen1.5-MoE-A2.7B
            - "deepseek": deepseek-moe-16b-base
            - "mixtral": Mixtral-8x7B-v0.1
            - None: è‡ªåŠ¨æ£€æµ‹
        log_every: æ—¥å¿—é—´éš” (steps)

    Example:
        ```python
        callback = get_moe_callback("olmoe", log_every=100)
        trainer = Trainer(model=model, callbacks=[callback], ...)
        ```
    """
    return MoEMonitorCallback(model_key=model_key, log_every=log_every, **kwargs)


# =============================================================================
# å‘åå…¼å®¹åˆ«å
# =============================================================================
UniversalMoECallback = MoEMonitorCallback
MoEMonitorCallbackV3 = MoEMonitorCallback
MoEMonitorCallbackV4 = MoEMonitorCallback


# =============================================================================
# é…ç½®é€ŸæŸ¥è¡¨
# =============================================================================
"""
æ¨¡å‹é…ç½®é€ŸæŸ¥è¡¨:

| æ¨¡å‹                  | ä¸“å®¶æ•° | Top-K | å±‚æ•° | Shared | Gate è·¯å¾„                    |
|-----------------------|--------|-------|------|--------|------------------------------|
| OLMoE-1B-7B-0125      | 64     | 8     | 16   | âŒ     | layers[i].mlp.gate           |
| Qwen1.5-MoE-A2.7B     | 60     | 4     | 24   | âœ… (1) | layers[i].mlp.gate           |
| deepseek-moe-16b-base | 64     | 6     | 28   | âœ… (2) | layers[i].mlp.gate           |
| Mixtral-8x7B-v0.1     | 8      | 2     | 32   | âŒ     | layers[i].block_sparse_moe.gate |

Config å±æ€§å¯¹ç…§:
- OLMoE:    config.num_experts=64, config.num_experts_per_tok=8
- Qwen:     config.num_experts=60, config.num_experts_per_tok=4
- DeepSeek: config.n_routed_experts=64, config.num_experts_per_tok=6
- Mixtral:  config.num_local_experts=8, config.num_experts_per_tok=2
"""
