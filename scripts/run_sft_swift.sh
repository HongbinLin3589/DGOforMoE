#!/bin/bash
# =============================================================================
# MS-Swift SFT Training Script for 4 MoE Models √ó 3 Datasets
# =============================================================================
# ‰ΩøÁî®ÊñπÊ≥ï:
#   ./run_sft_swift.sh MODEL_NAME DATASET_NAME
#
# Á§∫‰æã:
#   ./run_sft_swift.sh olmoe gsm8k
#   ./run_sft_swift.sh qwen math
#   ./run_sft_swift.sh deepseek mbpp
#   ./run_sft_swift.sh mixtral gsm8k
#
# Ê®°ÂûãÈÄâÈ°π: olmoe, qwen, deepseek, mixtral
# Êï∞ÊçÆÈõÜÈÄâÈ°π: gsm8k, math, mbpp
# =============================================================================

set -e

# =============================================================================
# Âä†ËΩΩÁéØÂ¢ÉÈÖçÁΩÆ
# =============================================================================
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
source "${SCRIPT_DIR}/env.sh"

# ÊøÄÊ¥ª conda ÁéØÂ¢É
activate_dgo_env

# ÂàõÂª∫ÂøÖË¶ÅÁõÆÂΩï
ensure_dirs

# =============================================================================
# Ê®°ÂûãÁâπÂÆöÈÖçÁΩÆ
# =============================================================================
# Êï∞ÊçÆÈõÜ max_length Êò†Â∞Ñ
declare -A MAX_LENGTH
MAX_LENGTH["gsm8k"]=1024
MAX_LENGTH["math"]=1024
MAX_LENGTH["mbpp"]=1024

# ÊåâÊ®°ÂûãÂ§ßÂ∞èË∞ÉÊï¥ batch size (‰øùÊåÅ global batch = 256)
declare -A MODEL_BATCH_SIZE
MODEL_BATCH_SIZE["olmoe"]=32      # Â∞èÊ®°ÂûãÔºåbatch=32, grad_accum=1 ‚Üí 32√ó1√ó8=256
MODEL_BATCH_SIZE["qwen"]=8        # ‰∏≠Ê®°ÂûãÔºåbatch=8, grad_accum=4 ‚Üí 8√ó4√ó8=256
MODEL_BATCH_SIZE["deepseek"]=8    # ‰∏≠Ê®°ÂûãÔºåbatch=8, grad_accum=4 ‚Üí 8√ó4√ó8=256
MODEL_BATCH_SIZE["mixtral"]=4     # Â§ßÊ®°ÂûãÔºåbatch=4, grad_accum=8 ‚Üí 4√ó8√ó8=256

declare -A MODEL_GRAD_ACCUM
MODEL_GRAD_ACCUM["olmoe"]=1
MODEL_GRAD_ACCUM["qwen"]=4
MODEL_GRAD_ACCUM["deepseek"]=4
MODEL_GRAD_ACCUM["mixtral"]=8

# ÊåâÊ®°ÂûãÂ§ßÂ∞èÈÄâÊã© deepspeed
declare -A MODEL_DEEPSPEED
MODEL_DEEPSPEED["olmoe"]="zero2"      # Â∞èÊ®°ÂûãÁî® zero2
MODEL_DEEPSPEED["qwen"]="zero2"       # ‰∏≠Ê®°ÂûãÁî® zero2
MODEL_DEEPSPEED["deepseek"]="zero3"   # ËæÉÂ§ßÊ®°ÂûãÁî® zero3
MODEL_DEEPSPEED["mixtral"]="zero3"    # Â§ßÊ®°ÂûãÁî® zero3

# =============================================================================
# ÂèÇÊï∞Ëß£Êûê
# =============================================================================
MODEL_KEY="${1:-olmoe}"
DATASET_KEY="${2:-gsm8k}"

# È™åËØÅÊ®°Âûã
MODEL_PATH=$(get_model_path "$MODEL_KEY")
if [[ -z "$MODEL_PATH" ]]; then
    echo "‚ùå Êú™Áü•Ê®°Âûã: $MODEL_KEY"
    echo "ÂèØÁî®Ê®°Âûã: olmoe, qwen, deepseek, mixtral"
    exit 1
fi

if [[ ! -d "$MODEL_PATH" ]]; then
    echo "‚ùå Ê®°ÂûãË∑ØÂæÑ‰∏çÂ≠òÂú®: $MODEL_PATH"
    exit 1
fi

MAX_LEN="${MAX_LENGTH[$DATASET_KEY]}"

# Êï∞ÊçÆÈõÜÂàóÊò†Â∞ÑÈÖçÁΩÆ
COLUMNS_MAPPING=""

case "$DATASET_KEY" in
    gsm8k)
        DATASET_PATH=$(get_dataset_path gsm8k)
        # GSM8K Êï∞ÊçÆÈõÜÂàóÂêç: question -> query, answer -> response
        COLUMNS_MAPPING='{"question":"query","answer":"response"}'
        ;;
    math)
        DATASET_PATH=$(get_dataset_path math)
        # MATH Êï∞ÊçÆÈõÜÂàóÂêç: problem -> query, solution -> response
        COLUMNS_MAPPING='{"problem":"query","solution":"response"}'
        ;;
    mbpp)
        DATASET_PATH=$(get_dataset_path mbpp)
        # MBPP Êï∞ÊçÆÈõÜÂàóÂêç: problem -> query, solution -> response
        COLUMNS_MAPPING='{"problem":"query","solution":"response"}'
        ;;
    *)
        echo "‚ùå Êú™Áü•Êï∞ÊçÆÈõÜ: $DATASET_KEY"
        echo "ÂèØÁî®Êï∞ÊçÆÈõÜ: gsm8k, math, mbpp"
        exit 1
        ;;
esac

OUTPUT_DIR="${SFT_OUTPUT}/${MODEL_KEY}_${DATASET_KEY}"
LOG_FILE="${SFT_LOGS}/${MODEL_KEY}_${DATASET_KEY}_$(date +%Y%m%d_%H%M%S).log"

# Á´ãÂç≥ÂàõÂª∫Êó•ÂøóÊñá‰ª∂Âπ∂ÂºÄÂßãËÆ∞ÂΩïÊâÄÊúâËæìÂá∫
exec > >(tee -a "$LOG_FILE") 2>&1
echo "Êó•ÂøóÊñá‰ª∂: $LOG_FILE"
echo "ÂºÄÂßãÊó∂Èó¥: $(date)"

# =============================================================================
# Ëé∑ÂèñÊ®°ÂûãÂØπÂ∫îÁöÑÈÖçÁΩÆ
# =============================================================================
BATCH_SIZE="${MODEL_BATCH_SIZE[$MODEL_KEY]:-16}"
GRADIENT_ACCUMULATION="${MODEL_GRAD_ACCUM[$MODEL_KEY]:-2}"
EVAL_BATCH_SIZE=$((BATCH_SIZE * 2))
DEEPSPEED="${MODEL_DEEPSPEED[$MODEL_KEY]:-zero2}"

# =============================================================================
# ÊâìÂç∞ÈÖçÁΩÆ
# =============================================================================
echo "============================================================"
echo "MS-Swift SFT Training Configuration"
echo "============================================================"
echo "È°πÁõÆÊ†πÁõÆÂΩï: $DGO_ROOT"
echo "Ê®°ÂûãË∑ØÂæÑ: $MODEL_PATH"
echo "Êï∞ÊçÆÈõÜ: $DATASET_PATH"
echo "ËæìÂá∫ÁõÆÂΩï: $OUTPUT_DIR"
echo "batch_size: $BATCH_SIZE"
echo "gradient_accumulation: $GRADIENT_ACCUMULATION"
echo "global_batch: $((BATCH_SIZE * GRADIENT_ACCUMULATION * NPROC_PER_NODE))"
echo "deepspeed: $DEEPSPEED"
echo ""
echo "LoRA ÈÖçÁΩÆ:"
echo "  rank: $DEFAULT_LORA_RANK"
echo "  alpha: $DEFAULT_LORA_ALPHA"
echo "  dropout: $DEFAULT_LORA_DROPOUT"
echo ""
echo "MoE ÈÖçÁΩÆ:"
echo "  router_aux_loss_coef: ${ROUTER_AUX_LOSS_COEF:-$DEFAULT_ROUTER_AUX_LOSS_COEF}"
echo "  moe_monitor_enabled: ${MOE_MONITOR_ENABLED:-$DEFAULT_MOE_MONITOR_ENABLED}"
echo "  moe_log_every: ${MOE_LOG_EVERY:-$DEFAULT_MOE_LOG_EVERY}"
echo "============================================================"

# =============================================================================
# ËøêË°å SFT ËÆ≠ÁªÉ
# =============================================================================
echo "üöÄ ÂºÄÂßãËÆ≠ÁªÉ..."

if [[ -n "$COLUMNS_MAPPING" ]]; then
    echo "  ÂàóÊò†Â∞Ñ: $COLUMNS_MAPPING"
fi

swift sft \
    --model "$MODEL_PATH" \
    --attn_impl sdpa \
    --dataset "$DATASET_PATH" \
    --output_dir "$OUTPUT_DIR" \
    ${COLUMNS_MAPPING:+--columns "$COLUMNS_MAPPING"} \
    \
    --train_type lora \
    --lora_rank $DEFAULT_LORA_RANK \
    --lora_alpha $DEFAULT_LORA_ALPHA \
    --lora_dropout $DEFAULT_LORA_DROPOUT \
    --target_modules all-linear \
    \
    --learning_rate $DEFAULT_LEARNING_RATE \
    --weight_decay $DEFAULT_WEIGHT_DECAY \
    --warmup_ratio $DEFAULT_WARMUP_RATIO \
    --adam_beta1 0.9 \
    --adam_beta2 0.95 \
    --lr_scheduler_type cosine \
    \
    --num_train_epochs $DEFAULT_NUM_EPOCHS \
    --per_device_train_batch_size $BATCH_SIZE \
    --per_device_eval_batch_size $EVAL_BATCH_SIZE \
    --gradient_accumulation_steps $GRADIENT_ACCUMULATION \
    --max_length $MAX_LEN \
    \
    --torch_dtype bfloat16 \
    --gradient_checkpointing true \
    \
    --save_strategy steps \
    --eval_strategy steps \
    --eval_steps 100 \
    --save_steps 100 \
    --save_total_limit 10 \
    --logging_steps 10 \
    \
    --dataloader_num_workers 4 \
    --dataset_num_proc 4 \
    \
    --use_hf true \
    --deepspeed $DEEPSPEED \
    \
    --report_to tensorboard \
    \
    --router_aux_loss_coef ${ROUTER_AUX_LOSS_COEF:-$DEFAULT_ROUTER_AUX_LOSS_COEF} \
    --moe_monitor_enabled ${MOE_MONITOR_ENABLED:-$DEFAULT_MOE_MONITOR_ENABLED} \
    --moe_log_every ${MOE_LOG_EVERY:-$DEFAULT_MOE_LOG_EVERY}

echo "‚úÖ SFT ËÆ≠ÁªÉÂÆåÊàê! ËæìÂá∫ÁõÆÂΩï: $OUTPUT_DIR"
echo "ÁªìÊùüÊó∂Èó¥: $(date)"
