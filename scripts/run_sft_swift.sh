#!/bin/bash
# =============================================================================
# MS-Swift SFT Training Script for 4 MoE Models Ã— 3 Datasets
# =============================================================================
# ä½¿ç”¨æ–¹æ³•:
#   ./run_sft_swift.sh MODEL_NAME DATASET_NAME
#
# ç¤ºä¾‹:
#   ./run_sft_swift.sh olmoe gsm8k
#   ./run_sft_swift.sh qwen math
#   ./run_sft_swift.sh deepseek mbpp
#   ./run_sft_swift.sh mixtral gsm8k
#
# æ¨¡å‹é€‰é¡¹: olmoe, qwen, deepseek, mixtral
# æ•°æ®é›†é€‰é¡¹: gsm8k, math, mbpp
# =============================================================================

set -e

# =============================================================================
# ç¯å¢ƒé…ç½®
# =============================================================================
export CUDA_VISIBLE_DEVICES="0,1,2,3,4,5,6,7"  # 8 GPUs

export HF_HOME="/usr/storage/fwan/huggingface_cache"
export HF_HUB_CACHE="/usr/storage/fwan/huggingface_cache/hub"
export HF_ENDPOINT="https://hf-mirror.com"
export USE_HF=1

export NPROC_PER_NODE=8  # å¿…é¡»ä¸ CUDA_VISIBLE_DEVICES æ•°é‡ä¸€è‡´
export MASTER_PORT=$((29500 + RANDOM % 100))  # éšæœºç«¯å£é¿å…å†²çª

source /opt/miniforge3/bin/activate dgo

# =============================================================================
# è·¯å¾„é…ç½®
# =============================================================================
BASE_DIR="/usr/commondata/public/hf_hub/cc/DGO"
OUTPUT_BASE="${BASE_DIR}/outputs/swift_sft"
LOG_DIR="${BASE_DIR}/logs/swift_sft"
HF_CACHE="/usr/storage/fwan/huggingface_cache/hub"
mkdir -p "${OUTPUT_BASE}" "${LOG_DIR}"

# æ•°æ®é›†è·¯å¾„
GSM8K_PATH="gsm8k"
MATH_PATH="${BASE_DIR}/ReDit/dataset/MATH/data/train/0000.parquet"
MBPP_DATASET="mbpp"

# =============================================================================
# æ¨¡å‹æ˜ å°„ - ä½¿ç”¨æœ¬åœ°ç¼“å­˜è·¯å¾„
# =============================================================================
declare -A MODEL_MAP
MODEL_MAP["olmoe"]="${HF_CACHE}/models--allenai--OLMoE-1B-7B-0125/snapshots/9b0c1aa87e34a20052389dce1f0cf01da783f654"
MODEL_MAP["qwen"]="${HF_CACHE}/models--Qwen--Qwen1.5-MoE-A2.7B/snapshots/1a758c50ecb6350748b9ce0a99d2352fd9fc11c9"
MODEL_MAP["deepseek"]="${HF_CACHE}/models--deepseek-ai--deepseek-moe-16b-base/snapshots/521d2bc4fb69a3f3ae565310fcc3b65f97af2580"
MODEL_MAP["mixtral"]="${HF_CACHE}/models--mistralai--Mixtral-8x7B-v0.1/snapshots/fc7ac94680e38d7348cfa806e51218e6273104b0"

# æ•°æ®é›† max_length æ˜ å°„
declare -A MAX_LENGTH
MAX_LENGTH["gsm8k"]=1024
MAX_LENGTH["math"]=1024
MAX_LENGTH["mbpp"]=1024

# =============================================================================
# è¶…å‚æ•°é…ç½®
# =============================================================================
LORA_RANK=8
LORA_ALPHA=64
LORA_DROPOUT=0.05
LEARNING_RATE=5e-6
WEIGHT_DECAY=0.1
WARMUP_RATIO=0.1
ADAM_BETA1=0.9
ADAM_BETA2=0.95
NUM_EPOCHS=35

# æŒ‰æ¨¡å‹å¤§å°è°ƒæ•´ batch size (ä¿æŒ global batch = 256)
declare -A MODEL_BATCH_SIZE
MODEL_BATCH_SIZE["olmoe"]=16      # å°æ¨¡å‹ï¼Œbatch=32, grad_accum=1 â†’ 32Ã—1Ã—8=256
MODEL_BATCH_SIZE["qwen"]=8       # ä¸­æ¨¡å‹ï¼Œbatch=16, grad_accum=2 â†’ 16Ã—2Ã—8=256
MODEL_BATCH_SIZE["deepseek"]=8   # ä¸­æ¨¡å‹ï¼Œbatch=16, grad_accum=2 â†’ 16Ã—2Ã—8=256
MODEL_BATCH_SIZE["mixtral"]=4    # å¤§æ¨¡å‹ï¼Œbatch=8, grad_accum=4 â†’ 8Ã—4Ã—8=256

declare -A MODEL_GRAD_ACCUM
MODEL_GRAD_ACCUM["olmoe"]=2
MODEL_GRAD_ACCUM["qwen"]=4
MODEL_GRAD_ACCUM["deepseek"]=4
MODEL_GRAD_ACCUM["mixtral"]=8

# æŒ‰æ¨¡å‹å¤§å°é€‰æ‹© deepspeed
declare -A MODEL_DEEPSPEED
MODEL_DEEPSPEED["olmoe"]="zero2"      # å°æ¨¡å‹ç”¨ zero2
MODEL_DEEPSPEED["qwen"]="zero2"       # ä¸­æ¨¡å‹ç”¨ zero2
MODEL_DEEPSPEED["deepseek"]="zero3"   # è¾ƒå¤§æ¨¡å‹ç”¨ zero3
MODEL_DEEPSPEED["mixtral"]="zero3"    # å¤§æ¨¡å‹ç”¨ zero3

# =============================================================================
# å‚æ•°è§£æ
# =============================================================================
MODEL_KEY="${1:-olmoe}"
DATASET_KEY="${2:-gsm8k}"

if [[ -z "${MODEL_MAP[$MODEL_KEY]}" ]]; then
    echo "âŒ æœªçŸ¥æ¨¡å‹: $MODEL_KEY"
    echo "å¯ç”¨æ¨¡å‹: olmoe, qwen, deepseek, mixtral"
    exit 1
fi

MODEL_PATH="${MODEL_MAP[$MODEL_KEY]}"

if [[ ! -d "$MODEL_PATH" ]]; then
    echo "âŒ æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: $MODEL_PATH"
    exit 1
fi

MAX_LEN="${MAX_LENGTH[$DATASET_KEY]}"

case "$DATASET_KEY" in
    gsm8k)
        DATASET_PATH="$GSM8K_PATH"
        ;;
    math)
        DATASET_PATH="$MATH_PATH"
        ;;
    mbpp)
        DATASET_PATH="$MBPP_DATASET"
        ;;
    *)
        echo "âŒ æœªçŸ¥æ•°æ®é›†: $DATASET_KEY"
        exit 1
        ;;
esac

OUTPUT_DIR="${OUTPUT_BASE}/${MODEL_KEY}_${DATASET_KEY}"
LOG_FILE="${LOG_DIR}/${MODEL_KEY}_${DATASET_KEY}_$(date +%Y%m%d_%H%M%S).log"

# ç«‹å³åˆ›å»ºæ—¥å¿—æ–‡ä»¶å¹¶å¼€å§‹è®°å½•æ‰€æœ‰è¾“å‡º
exec > >(tee -a "$LOG_FILE") 2>&1
echo "æ—¥å¿—æ–‡ä»¶: $LOG_FILE"
echo "å¼€å§‹æ—¶é—´: $(date)"

# =============================================================================
# è·å–æ¨¡å‹å¯¹åº”çš„é…ç½®
# =============================================================================
BATCH_SIZE="${MODEL_BATCH_SIZE[$MODEL_KEY]:-16}"
GRADIENT_ACCUMULATION="${MODEL_GRAD_ACCUM[$MODEL_KEY]:-2}"
EVAL_BATCH_SIZE=$((BATCH_SIZE * 2))
DEEPSPEED="${MODEL_DEEPSPEED[$MODEL_KEY]:-zero2}"

# =============================================================================
# æ‰“å°é…ç½®
# =============================================================================
echo "============================================================"
echo "MS-Swift SFT Training Configuration"
echo "============================================================"
echo "æ¨¡å‹è·¯å¾„: $MODEL_PATH"
echo "æ•°æ®é›†: $DATASET_PATH"
echo "è¾“å‡ºç›®å½•: $OUTPUT_DIR"
echo "batch_size: $BATCH_SIZE"
echo "gradient_accumulation: $GRADIENT_ACCUMULATION"
echo "global_batch: $((BATCH_SIZE * GRADIENT_ACCUMULATION * 8))"
echo "deepspeed: $DEEPSPEED"
echo "============================================================"

# =============================================================================
# è¿è¡Œ SFT è®­ç»ƒ (æŒ‰ç…§ README æ ¼å¼)
# =============================================================================
echo "ğŸš€ å¼€å§‹è®­ç»ƒ..."

swift sft \
    --model "$MODEL_PATH" \
    --attn_impl sdpa \
    --dataset "$DATASET_PATH" \
    --output_dir "$OUTPUT_DIR" \
    \
    --train_type lora \
    --lora_rank $LORA_RANK \
    --lora_alpha $LORA_ALPHA \
    --lora_dropout $LORA_DROPOUT \
    --target_modules all-linear \
    \
    --learning_rate $LEARNING_RATE \
    --weight_decay $WEIGHT_DECAY \
    --warmup_ratio $WARMUP_RATIO \
    --adam_beta1 $ADAM_BETA1 \
    --adam_beta2 $ADAM_BETA2 \
    --lr_scheduler_type cosine \
    \
    --num_train_epochs $NUM_EPOCHS \
    --per_device_train_batch_size $BATCH_SIZE \
    --per_device_eval_batch_size $EVAL_BATCH_SIZE \
    --gradient_accumulation_steps $GRADIENT_ACCUMULATION \
    --max_length $MAX_LEN \
    \
    --torch_dtype bfloat16 \
    --gradient_checkpointing true \
    \
    --save_strategy steps \
    --eval_strategy steps \
    --eval_steps 100 \
    --save_steps 100 \
    --save_total_limit 10 \
    --logging_steps 10 \
    \
    --dataloader_num_workers 4 \
    --dataset_num_proc 4 \
    \
    --use_hf true \
    --deepspeed $DEEPSPEED \
    \
    --report_to tensorboard

echo "âœ… SFT è®­ç»ƒå®Œæˆ! è¾“å‡ºç›®å½•: $OUTPUT_DIR"
echo "ç»“æŸæ—¶é—´: $(date)"
