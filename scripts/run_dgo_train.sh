#!/bin/bash
# =============================================================================
# DGO Training Script (Offline Weighted SFT)
# =============================================================================
# ä½¿ç”¨æ–¹æ³•:
#   bash run_dgo_train.sh [MODEL_NAME] [DATASET_NAME] [--freeze_router]
#
# ç¤ºä¾‹:
#   bash run_dgo_train.sh olmoe gsm8k              # Group C: å¯è®­ç»ƒrouter
#   bash run_dgo_train.sh olmoe gsm8k --freeze_router  # Group D: å†»ç»“router
#   bash run_dgo_train.sh qwen math
#   bash run_dgo_train.sh deepseek mbpp
#   bash run_dgo_train.sh mixtral gsm8k
#
# è¯´æ˜:
#   DGOè®­ç»ƒé˜¶æ®µæ˜¯offline weighted SFT
#   ä½¿ç”¨é¢„ç”Ÿæˆçš„æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œæ¯ä¸ªæ ·æœ¬æœ‰é¢„è®¡ç®—çš„æƒé‡
#   å‚æ•°å’ŒGRPOä¿æŒä¸€è‡´ (epochs=5, global_batch=256, ç­‰)
# =============================================================================

set -e

# =============================================================================
# ç¯å¢ƒé…ç½® (å’ŒGRPOä¸€è‡´)
# =============================================================================
export CUDA_VISIBLE_DEVICES="0,1,2,3,4,5,6,7"
export HF_HOME="/usr/storage/fwan/huggingface_cache"
export HF_HUB_CACHE="/usr/storage/fwan/huggingface_cache/hub"
export HF_ENDPOINT="https://hf-mirror.com"
export USE_HF=1
export NPROC_PER_NODE=8
export MASTER_PORT=$((29500 + RANDOM % 100))

# ç¦ç”¨Pythonè¾“å‡ºç¼“å†²ï¼Œç¡®ä¿æ—¥å¿—å®æ—¶åˆ·æ–° (é€‚åˆtmuxæŸ¥çœ‹)
export PYTHONUNBUFFERED=1

source /opt/miniforge3/bin/activate dgo

# =============================================================================
# è·¯å¾„é…ç½®
# =============================================================================
BASE_DIR="/usr/commondata/public/hf_hub/cc/DGO"
OUTPUT_BASE="${BASE_DIR}/outputs/swift_dgo"
LOG_DIR="${BASE_DIR}/logs/dgo_train"
HF_CACHE="/usr/storage/fwan/huggingface_cache/hub"
DGO_CACHE="${BASE_DIR}/dgo_cache"
mkdir -p "${OUTPUT_BASE}" "${LOG_DIR}"

# =============================================================================
# æ¨¡å‹æ˜ å°„ - å’ŒGRPOä¸€è‡´
# =============================================================================
declare -A MODEL_MAP
MODEL_MAP["olmoe"]="${HF_CACHE}/models--allenai--OLMoE-1B-7B-0125/snapshots/9b0c1aa87e34a20052389dce1f0cf01da783f654"
MODEL_MAP["qwen"]="${HF_CACHE}/models--Qwen--Qwen1.5-MoE-A2.7B/snapshots/1a758c50ecb6350748b9ce0a99d2352fd9fc11c9"
MODEL_MAP["deepseek"]="${HF_CACHE}/models--deepseek-ai--deepseek-moe-16b-base/snapshots/521d2bc4fb69a3f3ae565310fcc3b65f97af2580"
MODEL_MAP["mixtral"]="${HF_CACHE}/models--mistralai--Mixtral-8x7B-v0.1/snapshots/fc7ac94680e38d7348cfa806e51218e6273104b0"

# æ•°æ®é›† max_length æ˜ å°„ (å’ŒGRPOä¸€è‡´)
declare -A MAX_LENGTH
MAX_LENGTH["gsm8k"]=1024
MAX_LENGTH["math"]=1024
MAX_LENGTH["mbpp"]=1024

# =============================================================================
# è¶…å‚æ•°é…ç½® - å’ŒGRPOå®Œå…¨ä¸€è‡´
# =============================================================================
LORA_RANK=8
LORA_ALPHA=64
LORA_DROPOUT=0.05
LEARNING_RATE=5e-6
WEIGHT_DECAY=0.1
WARMUP_RATIO=0.1
ADAM_BETA1=0.9
ADAM_BETA2=0.95
NUM_EPOCHS=5  # å’ŒGRPOä¸€è‡´

# æŒ‰æ¨¡å‹å¤§å°è°ƒæ•´ batch size (ä¿æŒ global batch = 256, å’ŒGRPOä¸€è‡´)
declare -A MODEL_BATCH_SIZE
MODEL_BATCH_SIZE["olmoe"]=32      # 16Ã—2Ã—8=256
MODEL_BATCH_SIZE["qwen"]=32        # 8Ã—4Ã—8=256
MODEL_BATCH_SIZE["deepseek"]=32    # 8Ã—4Ã—8=256
MODEL_BATCH_SIZE["mixtral"]=32     # 2Ã—16Ã—8=256

declare -A MODEL_GRAD_ACCUM
MODEL_GRAD_ACCUM["olmoe"]=1
MODEL_GRAD_ACCUM["qwen"]=1
MODEL_GRAD_ACCUM["deepseek"]=1
MODEL_GRAD_ACCUM["mixtral"]=1

# DeepSpeedé…ç½® - å’ŒGRPOä¸€è‡´ (ç”¨zero2æ›´ç¨³å®š)
declare -A MODEL_DEEPSPEED
MODEL_DEEPSPEED["olmoe"]="zero2"
MODEL_DEEPSPEED["qwen"]="zero3"
MODEL_DEEPSPEED["deepseek"]="zero3"
MODEL_DEEPSPEED["mixtral"]="zero3"

# =============================================================================
# DGOç‰¹å®šå‚æ•°
# =============================================================================
# æ³¨æ„: DGOçš„betaå’ŒGRPOçš„betaå«ä¹‰ä¸åŒ
# - GRPO beta=0.01: KLæ•£åº¦æƒ©ç½šç³»æ•°
# - DGO beta=0.1: æƒé‡æ¸©åº¦å‚æ•° (w = exp(r/beta) / Z)
#   betaè¶Šå°ï¼Œæƒé‡åˆ†å¸ƒè¶Šå°–é”ï¼ˆé«˜rewardæ ·æœ¬æƒé‡æ›´å¤§ï¼‰
DGO_BETA=0.1

# =============================================================================
# å‚æ•°è§£æ
# =============================================================================
MODEL_KEY="${1:-olmoe}"
DATASET_KEY="${2:-gsm8k}"
FREEZE_ROUTER=""

# æ£€æŸ¥æ˜¯å¦æœ‰--freeze_routerå‚æ•°
for arg in "$@"; do
    if [[ "$arg" == "--freeze_router" ]]; then
        FREEZE_ROUTER="--freeze_router true"
        echo "ğŸ“Œ å¯ç”¨routerå†»ç»“ (Group Då®éªŒ)"
    fi
done

if [[ -z "${MODEL_MAP[$MODEL_KEY]}" ]]; then
    echo "âŒ æœªçŸ¥æ¨¡å‹: $MODEL_KEY"
    echo "å¯ç”¨æ¨¡å‹: olmoe, qwen, deepseek, mixtral"
    exit 1
fi

MODEL_PATH="${MODEL_MAP[$MODEL_KEY]}"

if [[ ! -d "$MODEL_PATH" ]]; then
    echo "âŒ æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: $MODEL_PATH"
    exit 1
fi

# DGOæ•°æ®æ–‡ä»¶
DGO_DATA_FILE="${DGO_CACHE}/dgo_data_${MODEL_KEY}_${DATASET_KEY}.json"

if [[ ! -f "$DGO_DATA_FILE" ]]; then
    echo "âŒ DGOæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: $DGO_DATA_FILE"
    echo "è¯·å…ˆè¿è¡Œ: bash scripts/run_dgo_gen.sh $MODEL_KEY $DATASET_KEY"
    exit 1
fi

# è·å–æ¨¡å‹é…ç½®
MAX_LEN="${MAX_LENGTH[$DATASET_KEY]}"
BATCH_SIZE="${MODEL_BATCH_SIZE[$MODEL_KEY]}"
GRADIENT_ACCUMULATION="${MODEL_GRAD_ACCUM[$MODEL_KEY]}"
DEEPSPEED="${MODEL_DEEPSPEED[$MODEL_KEY]}"

# è¾“å‡ºé…ç½®
if [[ -n "$FREEZE_ROUTER" ]]; then
    OUTPUT_DIR="${OUTPUT_BASE}/${MODEL_KEY}_${DATASET_KEY}_frozen"
else
    OUTPUT_DIR="${OUTPUT_BASE}/${MODEL_KEY}_${DATASET_KEY}"
fi
LOG_FILE="${LOG_DIR}/${MODEL_KEY}_${DATASET_KEY}_$(date +%Y%m%d_%H%M%S).log"

mkdir -p "$(dirname "$LOG_FILE")"

# =============================================================================
# å¼€å§‹è®°å½•æ—¥å¿—
# =============================================================================
exec > >(tee -a "$LOG_FILE") 2>&1

GLOBAL_BATCH=$((BATCH_SIZE * GRADIENT_ACCUMULATION * 8))

echo "============================================================"
echo "DGO Training (Offline Weighted SFT)"
echo "============================================================"
echo "æ¨¡å‹è·¯å¾„: $MODEL_PATH"
echo "æ•°æ®é›†: $DATASET_KEY"
echo "DGOæ•°æ®: $DGO_DATA_FILE"
echo "è¾“å‡ºç›®å½•: $OUTPUT_DIR"
echo ""
echo "è®­ç»ƒé…ç½® (å’ŒGRPOä¸€è‡´):"
echo "  num_epochs: $NUM_EPOCHS"
echo "  per_device_batch_size: $BATCH_SIZE"
echo "  gradient_accumulation: $GRADIENT_ACCUMULATION"
echo "  global_batch_size: $GLOBAL_BATCH"
echo "  learning_rate: $LEARNING_RATE"
echo "  deepspeed: $DEEPSPEED"
echo ""
echo "DGOé…ç½®:"
echo "  dgo_beta: $DGO_BETA"
echo "  freeze_router: ${FREEZE_ROUTER:-false}"
echo ""
echo "LoRAé…ç½®:"
echo "  rank: $LORA_RANK"
echo "  alpha: $LORA_ALPHA"
echo "  dropout: $LORA_DROPOUT"
echo "============================================================"
echo "å¼€å§‹æ—¶é—´: $(date)"
echo ""

# =============================================================================
# è¿è¡ŒDGOè®­ç»ƒ
# =============================================================================
echo "ğŸš€ å¼€å§‹DGOè®­ç»ƒ..."

swift rlhf \
    --rlhf_type dgo \
    --model "$MODEL_PATH" \
    --template default \
    --template_backend swift \
    --output_dir "$OUTPUT_DIR" \
    \
    --dgo_data_file "$DGO_DATA_FILE" \
    --dgo_beta "$DGO_BETA" \
    --reward_funcs accuracy \
    $FREEZE_ROUTER \
    \
    --train_type lora \
    --lora_rank $LORA_RANK \
    --lora_alpha $LORA_ALPHA \
    --lora_dropout $LORA_DROPOUT \
    --target_modules all-linear \
    \
    --torch_dtype bfloat16 \
    --attn_impl sdpa \
    --max_length $MAX_LEN \
    --max_completion_length $MAX_LEN \
    --overlong_filter true \
    \
    --num_train_epochs $NUM_EPOCHS \
    --per_device_train_batch_size $BATCH_SIZE \
    --learning_rate $LEARNING_RATE \
    --weight_decay $WEIGHT_DECAY \
    --adam_beta1 $ADAM_BETA1 \
    --adam_beta2 $ADAM_BETA2 \
    --gradient_accumulation_steps $GRADIENT_ACCUMULATION \
    --lr_scheduler_type cosine \
    --warmup_ratio $WARMUP_RATIO \
    \
    --save_strategy steps \
    --eval_strategy steps \
    --eval_steps 100 \
    --save_steps 100 \
    --save_total_limit 10 \
    --logging_steps 10 \
    --dataloader_num_workers 4 \
    \
    --deepspeed $DEEPSPEED \
    --report_to tensorboard

echo ""
echo "============================================================"
echo "âœ… DGOè®­ç»ƒå®Œæˆ!"
echo "æ¨¡å‹ä¿å­˜åˆ°: $OUTPUT_DIR"
echo "æ—¥å¿—æ–‡ä»¶: $LOG_FILE"
echo "ç»“æŸæ—¶é—´: $(date)"
echo "============================================================"

# æ£€æŸ¥è¾“å‡ºç›®å½•
if [ -d "$OUTPUT_DIR" ]; then
    NUM_CHECKPOINTS=$(find "$OUTPUT_DIR" -maxdepth 1 -name "checkpoint-*" 2>/dev/null | wc -l)
    echo "âœ… ä¿å­˜äº† $NUM_CHECKPOINTS ä¸ªcheckpoints"

    LAST_CKPT=$(ls -d "$OUTPUT_DIR"/checkpoint-* 2>/dev/null | tail -1)
    if [ -n "$LAST_CKPT" ]; then
        echo "ğŸ“ æœ€æ–°checkpoint: $LAST_CKPT"
    fi
else
    echo "âš ï¸ è¾“å‡ºç›®å½•æœªæ‰¾åˆ°: $OUTPUT_DIR"
fi

echo ""
echo "ä¸‹ä¸€æ­¥:"
echo "1. è¯„ä¼°æ¨¡å‹: lm-eval run --model hf --model_args pretrained=$OUTPUT_DIR --tasks gsm8k"
echo "2. åˆå¹¶LoRA: swift export --ckpt_dir $OUTPUT_DIR --merge_lora true"
