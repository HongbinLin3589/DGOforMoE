#!/bin/bash
# =============================================================================
# DGO Data Generation Script (Inference Only)
# =============================================================================
# ä½¿ç”¨æ–¹æ³•:
#   bash run_dgo_gen.sh [MODEL_NAME] [DATASET_NAME]
#
# ç¤ºä¾‹:
#   bash run_dgo_gen.sh olmoe gsm8k
#   bash run_dgo_gen.sh qwen math
#   bash run_dgo_gen.sh deepseek mbpp
#   bash run_dgo_gen.sh mixtral gsm8k
#
# è¯´æ˜:
#   DGOç”Ÿæˆé˜¶æ®µåªéœ€è¦inferenceï¼Œä¸éœ€è¦è®­ç»ƒ
#   ä½¿ç”¨vLLMè¿›è¡Œå¿«é€Ÿæ¨ç†ï¼Œä¸ºæ¯ä¸ªpromptç”ŸæˆNä¸ªresponse
# =============================================================================

set -e

# =============================================================================
# ç¯å¢ƒé…ç½®
# =============================================================================
export CUDA_VISIBLE_DEVICES="0,1,2,3,4,5,6,7"
export HF_HOME="/usr/storage/fwan/huggingface_cache"
export HF_HUB_CACHE="/usr/storage/fwan/huggingface_cache/hub"
export HF_ENDPOINT="https://hf-mirror.com"
export USE_HF=1

# ç¦ç”¨Pythonè¾“å‡ºç¼“å†²ï¼Œç¡®ä¿æ—¥å¿—å®æ—¶åˆ·æ–° (é€‚åˆtmuxæŸ¥çœ‹)
export PYTHONUNBUFFERED=1

# ç¦ç”¨FlashInfer sampler (é¿å…JITç¼–è¯‘é—®é¢˜ï¼Œä½¿ç”¨vLLMå†…ç½®å®ç°)
export VLLM_USE_FLASHINFER_SAMPLER=0

source /opt/miniforge3/bin/activate dgo

# =============================================================================
# è·¯å¾„é…ç½®
# =============================================================================
BASE_DIR="/usr/commondata/public/hf_hub/cc/DGO"
OUTPUT_BASE="${BASE_DIR}/dgo_cache"
LOG_DIR="${BASE_DIR}/logs/dgo_gen"
HF_CACHE="/usr/storage/fwan/huggingface_cache/hub"
mkdir -p "${OUTPUT_BASE}" "${LOG_DIR}"

# =============================================================================
# æ¨¡å‹æ˜ å°„ - ä½¿ç”¨æœ¬åœ°ç¼“å­˜è·¯å¾„ (å’ŒGRPOä¸€è‡´)
# =============================================================================
declare -A MODEL_MAP
MODEL_MAP["olmoe"]="${HF_CACHE}/models--allenai--OLMoE-1B-7B-0125/snapshots/9b0c1aa87e34a20052389dce1f0cf01da783f654"
MODEL_MAP["qwen"]="${HF_CACHE}/models--Qwen--Qwen1.5-MoE-A2.7B/snapshots/1a758c50ecb6350748b9ce0a99d2352fd9fc11c9"
MODEL_MAP["deepseek"]="${HF_CACHE}/models--deepseek-ai--deepseek-moe-16b-base/snapshots/521d2bc4fb69a3f3ae565310fcc3b65f97af2580"
MODEL_MAP["mixtral"]="${HF_CACHE}/models--mistralai--Mixtral-8x7B-v0.1/snapshots/fc7ac94680e38d7348cfa806e51218e6273104b0"

# =============================================================================
# vLLM é…ç½®æ˜ å°„ - å’ŒGRPOä¸€è‡´
# =============================================================================
declare -A VLLM_TP_SIZE
VLLM_TP_SIZE["olmoe"]=8    # å°æ¨¡å‹ç”¨8å¡åŠ é€Ÿ
VLLM_TP_SIZE["qwen"]=8     # ä¸­æ¨¡å‹ç”¨8å¡åŠ é€Ÿ
VLLM_TP_SIZE["deepseek"]=8 # ä¸­å¤§æ¨¡å‹ç”¨8å¡
VLLM_TP_SIZE["mixtral"]=8  # å¤§æ¨¡å‹(46.7B)éœ€è¦8å¡

declare -A VLLM_MEM_UTIL
VLLM_MEM_UTIL["olmoe"]=0.9
VLLM_MEM_UTIL["qwen"]=0.9
VLLM_MEM_UTIL["deepseek"]=0.9
VLLM_MEM_UTIL["mixtral"]=0.9

# æ•°æ®é›† max_length æ˜ å°„ (å’ŒGRPOä¸€è‡´)
declare -A MAX_LENGTH
MAX_LENGTH["gsm8k"]=1024
MAX_LENGTH["math"]=1024
MAX_LENGTH["mbpp"]=1024

# =============================================================================
# DGOç”Ÿæˆå‚æ•° (å’ŒGRPOçš„num_generationsä¸€è‡´)
# =============================================================================
NUM_GENERATIONS=8
TEMPERATURE=1.0  # å’ŒGRPOä¸€è‡´
TOP_P=0.95

# =============================================================================
# æ‰¹é‡æ¨ç†ä¼˜åŒ–å‚æ•° (å……åˆ†åˆ©ç”¨æ˜¾å­˜ï¼Œä½†é¿å…OOM)
# =============================================================================
# max_num_seqs: å¹¶è¡Œå¤„ç†çš„æœ€å¤§åºåˆ—æ•°
#   - æ¯ä¸ªpromptç”Ÿæˆ8ä¸ªåºåˆ—ï¼Œæ‰€ä»¥å®é™…å¹¶è¡Œ = max_num_seqs / 8 ä¸ªprompts
#   - å¢å¤§å¯æé«˜ååé‡ï¼Œä½†éœ€è¦æ›´å¤šæ˜¾å­˜
#   - æ ¹æ®Copilot Reviewé™ä½é…ç½®ä»¥é¿å…OOM
declare -A MAX_NUM_SEQS
MAX_NUM_SEQS["olmoe"]=256     # å°æ¨¡å‹ (1.3Bæ¿€æ´»)ï¼Œå¹¶è¡Œ32 prompts
MAX_NUM_SEQS["qwen"]=128      # ä¸­æ¨¡å‹ (2.7Bæ¿€æ´»)ï¼Œå¹¶è¡Œ16 prompts
MAX_NUM_SEQS["deepseek"]=128  # ä¸­æ¨¡å‹ (2.8Bæ¿€æ´», TP=2)ï¼Œå¹¶è¡Œ16 prompts
MAX_NUM_SEQS["mixtral"]=64    # å¤§æ¨¡å‹ (12.9Bæ¿€æ´», TP=2)ï¼Œå¹¶è¡Œ8 prompts

# max_num_batched_tokens: æ¯æ‰¹æœ€å¤§tokenæ•°ï¼Œæ›´ç²¾ç»†çš„æ˜¾å­˜æ§åˆ¶
declare -A MAX_BATCHED_TOKENS
MAX_BATCHED_TOKENS["olmoe"]=16384     # å°æ¨¡å‹å¯ä»¥å¤„ç†æ›´å¤štokens
MAX_BATCHED_TOKENS["qwen"]=12288      # ä¸­æ¨¡å‹
MAX_BATCHED_TOKENS["deepseek"]=10240  # è¾ƒå¤§æ¨¡å‹
MAX_BATCHED_TOKENS["mixtral"]=8192    # å¤§æ¨¡å‹

# swap_space: CPU swapç©ºé—´(GB)ï¼Œç”¨äºæ”¯æŒæ›´å¤§batch
SWAP_SPACE=8

# =============================================================================
# å‚æ•°è§£æ
# =============================================================================
MODEL_KEY="${1:-olmoe}"
DATASET_KEY="${2:-gsm8k}"

if [[ -z "${MODEL_MAP[$MODEL_KEY]}" ]]; then
    echo "âŒ æœªçŸ¥æ¨¡å‹: $MODEL_KEY"
    echo "å¯ç”¨æ¨¡å‹: olmoe, qwen, deepseek, mixtral"
    exit 1
fi

MODEL_PATH="${MODEL_MAP[$MODEL_KEY]}"

if [[ ! -d "$MODEL_PATH" ]]; then
    echo "âŒ æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: $MODEL_PATH"
    exit 1
fi

VLLM_TP="${VLLM_TP_SIZE[$MODEL_KEY]}"
VLLM_MEM="${VLLM_MEM_UTIL[$MODEL_KEY]}"
MAX_LEN="${MAX_LENGTH[$DATASET_KEY]}"
MAX_SEQS="${MAX_NUM_SEQS[$MODEL_KEY]}"
MAX_TOKENS_BATCH="${MAX_BATCHED_TOKENS[$MODEL_KEY]}"

# è¾“å‡ºé…ç½®
OUTPUT_FILE="${OUTPUT_BASE}/dgo_data_${MODEL_KEY}_${DATASET_KEY}.json"
LOG_FILE="${LOG_DIR}/${MODEL_KEY}_${DATASET_KEY}_$(date +%Y%m%d_%H%M%S).log"

mkdir -p "$(dirname "$LOG_FILE")"

# =============================================================================
# å¼€å§‹è®°å½•æ—¥å¿—
# =============================================================================
exec > >(tee -a "$LOG_FILE") 2>&1

echo "============================================================"
echo "DGO Data Generation (Inference Only)"
echo "============================================================"
echo "æ¨¡å‹è·¯å¾„: $MODEL_PATH"
echo "æ•°æ®é›†: $DATASET_KEY"
echo "è¾“å‡ºæ–‡ä»¶: $OUTPUT_FILE"
echo ""
echo "vLLMé…ç½®:"
echo "  tensor_parallel_size: $VLLM_TP"
echo "  gpu_memory_utilization: $VLLM_MEM"
echo "  max_tokens: $MAX_LEN"
echo "  max_num_seqs: $MAX_SEQS (å¹¶è¡Œåºåˆ—æ•°)"
echo "  max_num_batched_tokens: $MAX_TOKENS_BATCH"
echo "  swap_space: ${SWAP_SPACE}GB"
echo ""
echo "ç”Ÿæˆé…ç½®:"
echo "  num_generations: $NUM_GENERATIONS"
echo "  temperature: $TEMPERATURE"
echo "  top_p: $TOP_P"
echo "  å¹¶è¡Œprompts: $((MAX_SEQS / NUM_GENERATIONS)) (=$MAX_SEQS / $NUM_GENERATIONS)"
echo "============================================================"
echo "å¼€å§‹æ—¶é—´: $(date)"
echo ""

# =============================================================================
# è¿è¡ŒvLLMæ¨ç† (åªåšinferenceï¼Œä¸è®­ç»ƒ)
# =============================================================================
echo "ğŸš€ å¼€å§‹DGOæ•°æ®ç”Ÿæˆ (vLLM inference)..."

python "${BASE_DIR}/vllm_inference.py" \
    --model_name "$MODEL_PATH" \
    --dataset "$DATASET_KEY" \
    --dataset_split train \
    --n "$NUM_GENERATIONS" \
    --temperature "$TEMPERATURE" \
    --top_p "$TOP_P" \
    --max_tokens "$MAX_LEN" \
    --tensor_parallel_size "$VLLM_TP" \
    --gpu_memory_utilization "$VLLM_MEM" \
    --max_num_seqs "$MAX_SEQS" \
    --max_num_batched_tokens "$MAX_TOKENS_BATCH" \
    --swap_space "$SWAP_SPACE" \
    --output_file "$OUTPUT_FILE" \
    --stop $'\n### Human:' --stop '</answer>'

echo ""
echo "============================================================"
echo "âœ… DGOæ•°æ®ç”Ÿæˆå®Œæˆ!"
echo "è¾“å‡ºæ–‡ä»¶: $OUTPUT_FILE"
echo "æ—¥å¿—æ–‡ä»¶: $LOG_FILE"
echo "ç»“æŸæ—¶é—´: $(date)"
echo "============================================================"

# æ£€æŸ¥è¾“å‡ºæ–‡ä»¶
if [ -f "$OUTPUT_FILE" ]; then
    NUM_SAMPLES=$(python -c "import json; print(len(json.load(open('$OUTPUT_FILE'))))")
    echo "âœ… ç”Ÿæˆ $NUM_SAMPLES ä¸ªprompts Ã— $NUM_GENERATIONS generations = $((NUM_SAMPLES * NUM_GENERATIONS)) ä¸ªæ ·æœ¬"
else
    echo "âŒ è¾“å‡ºæ–‡ä»¶ä¸å­˜åœ¨: $OUTPUT_FILE"
    exit 1
fi

echo ""
echo "ä¸‹ä¸€æ­¥: è¿è¡ŒDGOè®­ç»ƒ"
echo "  bash scripts/run_dgo_train.sh $MODEL_KEY $DATASET_KEY"
